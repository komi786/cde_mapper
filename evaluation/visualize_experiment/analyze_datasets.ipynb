{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv, json\n",
    "# from rag.data_loader import *\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(data_dir, filter_composite, filter_duplicate):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        a path of data\n",
    "    filter_composite : bool\n",
    "        filter composite mentions\n",
    "    filter_duplicate : bool\n",
    "        filter duplicate queries\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : np.array\n",
    "        mention, cui pairs\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # concept_files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "    file_types = (\"*.concept\", \"*.txt\")\n",
    "    concept_files = []\n",
    "    for ft in file_types:\n",
    "        concept_files.extend(glob.glob(os.path.join(data_dir, ft)))\n",
    "\n",
    "    for concept_file in tqdm(concept_files):\n",
    "        with open(concept_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            concepts = f.readlines()\n",
    "\n",
    "        for concept in concepts:\n",
    "            concept = concept.split(\"||\")\n",
    "            if len(concept) == 2:\n",
    "                mention = concept[1].strip().lower()\n",
    "                cui = str(concept[0].strip())\n",
    "                if cui.lower() == \"cui-less\":\n",
    "                    continue\n",
    "                is_composite = cui.replace(\"+\", \"|\").count(\"|\") > 0\n",
    "\n",
    "                if filter_composite and is_composite:\n",
    "                    continue\n",
    "                else:\n",
    "                    data.append((mention, cui))\n",
    "            elif len(concept) == 6:\n",
    "                mention = concept[3].strip().lower()\n",
    "                cui = concept[4].strip()\n",
    "                if cui.lower() == \"cui-less\":\n",
    "                    continue\n",
    "                is_composite = cui.replace(\"+\", \"|\").count(\"|\") > 0\n",
    "                context = concept[5].strip()\n",
    "                if filter_composite and is_composite:\n",
    "                    continue\n",
    "                else:\n",
    "                    data.append((mention, context, cui))\n",
    "\n",
    "    if filter_duplicate:\n",
    "        data = list(dict.fromkeys(data))\n",
    "    print(f\"length of queries = {len(data)}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCBI DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of queries = 48714\n",
      "domains in ncbi are  = ['Measurement', 'Condition', 'Observation', 'Device', 'Procedure']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "concepts_df = pd.read_csv(\n",
    "    \"data/output/concepts.csv\", dtype=str\n",
    ")\n",
    "concept_dict_to_name = concepts_df.set_index(\"concept_id\")[\"domain_id\"].to_dict()\n",
    "\n",
    "ncbi_directory = \"data/eval_datasets/original_ncbi-disease\"\n",
    "ncbi_data = load_data(ncbi_directory, filter_composite=True, filter_duplicate=True)\n",
    "domain = []\n",
    "for item in ncbi_data:\n",
    "    _, cui = item\n",
    "    if cui in concept_dict_to_name:\n",
    "        if concept_dict_to_name[cui] not in domain:\n",
    "            domain.append(concept_dict_to_name[cui])\n",
    "print(f\"domains in ncbi are  = {domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC5CDR-Disease DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of queries = 49096\n",
      "domains in bc5cdr-D are  = ['Measurement', 'Condition', 'Observation', 'Device', 'Procedure']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For BC5CDR\n",
    "ncbi_directory = \"data/eval_datasets/original_bc5cdr-disease\"\n",
    "ncbi_data = load_data(ncbi_directory, filter_composite=True, filter_duplicate=True)\n",
    "domain = []\n",
    "for item in ncbi_data:\n",
    "    _, cui = item\n",
    "    if cui in concept_dict_to_name:\n",
    "        if concept_dict_to_name[cui] not in domain:\n",
    "            domain.append(concept_dict_to_name[cui])\n",
    "print(f\"domains in bc5cdr-D are  = {domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC5CDR-Disease CHEMICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncbi_directory = \"/workspace/rag_pipeline/syn_map/eval_dataset/bc5cdr-chemical\"\n",
    "# ncbi_data = load_data(ncbi_directory, filter_composite=True, filter_duplicate=True)\n",
    "# domain = []\n",
    "# for item in ncbi_data:\n",
    "#     _, cui = item\n",
    "#     if cui in concept_dict_to_name:\n",
    "#         if concept_dict_to_name[cui] not in domain:\n",
    "#             domain.append(concept_dict_to_name[cui])\n",
    "# print(f\"domains in bc5cdr-C are  = {domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedMention Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_med_mentiondata(\n",
    "#     data_dir, cui_to_concept_id, load_full_sentence=False, filter_duplicate=True\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data_dir : str\n",
    "#         a path of data\n",
    "#     filter_composite : bool\n",
    "#         filter composite mentions\n",
    "#     filter_duplicate : bool\n",
    "#         filter duplicate queries\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     data : np.array\n",
    "#         mention, cui pairs\n",
    "#     \"\"\"\n",
    "#     data = []\n",
    "#     file_types = (\"*.concept\", \"*.txt\")\n",
    "#     concept_files = []\n",
    "#     for ft in file_types:\n",
    "#         concept_files.extend(glob.glob(os.path.join(data_dir, ft)))\n",
    "\n",
    "#     for concept_file in tqdm(concept_files):\n",
    "#         # concept_files = glob.glob(os.path.join(data_dir, \"*.concept\"))\n",
    "#         with open(concept_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#             lines = f.readlines()\n",
    "#             for line in lines:\n",
    "#                 line = line.split(\"||\")\n",
    "#                 if len(line) != 5:\n",
    "#                     continue\n",
    "#                 cui = str(line[4]).strip()\n",
    "#                 if cui in cui_to_concept_id:\n",
    "#                     cui = cui_to_concept_id[cui]\n",
    "#                     mention = line[3]\n",
    "#                     data.append((mention, cui))\n",
    "#                 else:\n",
    "#                     continue\n",
    "\n",
    "#             # Ensures mentions is not empty\n",
    "\n",
    "#             # print(mentions)\n",
    "#             # if mentions:\n",
    "#             #     for mention in mentions:\n",
    "#             #         print(mention)\n",
    "#             #         data.append((sentence, mention, cui))\n",
    "\n",
    "#     if filter_duplicate:\n",
    "#         data = list(dict.fromkeys(data))\n",
    "#     print(\"query size:\", len(data))\n",
    "#     return data\n",
    "\n",
    "\n",
    "# medmention_dir = \"/workspace/rag_pipeline/syn_map/eval_dataset/PMedMention/full\"\n",
    "\n",
    "# ohdsi_to_cui = pd.read_csv(\n",
    "#     \"/workspace/rag_pipeline/data/input/omop_v5.4/CUItoOHDSI.csv\", dtype=str\n",
    "# )\n",
    "# cui_to_concept_id = ohdsi_to_cui.set_index(\"CUI\")[\"concept_id\"].to_dict()\n",
    "# medmention_data = load_med_mentiondata(medmention_dir, cui_to_concept_id)\n",
    "# domain = []\n",
    "# for item in medmention_data:\n",
    "#     _, cui = item\n",
    "#     if cui in concept_dict_to_name:\n",
    "#         if concept_dict_to_name[cui] not in domain:\n",
    "#             domain.append(concept_dict_to_name[cui])\n",
    "# print(f\"domains in MedMention are  = {domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cometa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cometa_directory = \"/workspace/rag_pipeline/syn_map/eval_dataset/cometa\"\n",
    "# cometa_data = load_data(cometa_directory, filter_composite=True, filter_duplicate=True)\n",
    "# domain = []\n",
    "# for item in cometa_data:\n",
    "#     _, cui = item\n",
    "#     if cui in concept_dict_to_name:\n",
    "#         if concept_dict_to_name[cui] not in domain:\n",
    "#             domain.append(concept_dict_to_name[cui])\n",
    "# print(f\"domains in ncbi are  = {domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AskaPatient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# askapatient_directory = \"/workspace/rag_pipeline/syn_map/eval_dataset/askapatient\"\n",
    "# askapatient_data = load_data(\n",
    "#     askapatient_directory, filter_composite=True, filter_duplicate=True\n",
    "# )\n",
    "# domain = []\n",
    "# for item in askapatient_data:\n",
    "#     _, cui = item\n",
    "#     if cui in concept_dict_to_name:\n",
    "#         if concept_dict_to_name[cui] not in domain:\n",
    "#             domain.append(concept_dict_to_name[cui])\n",
    "# print(f\"domains in ncbi are  = {domain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def load_jsonl(file_path: str) -> list:\n",
    "#     \"\"\"\n",
    "#     Loads a .jsonl file into a list of dictionaries.\n",
    "\n",
    "#     Parameters:\n",
    "#         file_path (str): The path to the .jsonl file.\n",
    "\n",
    "#     Returns:\n",
    "#         list: A list of dictionaries representing the .jsonl content.\n",
    "#     \"\"\"\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         return [json.loads(line) for line in f]\n",
    "\n",
    "# def save_jsonl(file_path: str, data: list):\n",
    "#     \"\"\"\n",
    "#     Saves a list of dictionaries into a .jsonl file.\n",
    "\n",
    "#     Parameters:\n",
    "#         file_path (str): The path to the .jsonl file.\n",
    "#         data (list): A list of dictionaries to be saved.\n",
    "#     \"\"\"\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         for entry in data:\n",
    "#             f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# def append_jsonl(source_file: str, destination_file: str):\n",
    "#     \"\"\"\n",
    "#     Appends the content of source_file to the end of destination_file, avoiding duplicates.\n",
    "\n",
    "#     Parameters:\n",
    "#         source_file (str): The path to the source .jsonl file.\n",
    "#         destination_file (str): The path to the destination .jsonl file.\n",
    "#     \"\"\"\n",
    "#     source_data = load_jsonl(source_file)\n",
    "#     destination_data = load_jsonl(destination_file)\n",
    "\n",
    "#     destination_content = {json.dumps(entry) for entry in destination_data}\n",
    "\n",
    "#     for entry in source_data:\n",
    "#         entry_json = json.dumps(entry)\n",
    "#         if entry_json not in destination_content:\n",
    "#             destination_data.append(entry)\n",
    "#             destination_content.add(entry_json)\n",
    "#     destination_file = '/workspace/rag_pipeline/data/output/concepts_all_v1.jsonl'\n",
    "#     save_jsonl(destination_file, destination_data)\n",
    "\n",
    "# # Usage example\n",
    "# destination_file_path = '/workspace/rag_pipeline/data/output/concepts_all.jsonl'\n",
    "# source_file_path= '/workspace/rag_pipeline/data/output/medra_concepts.jsonl'\n",
    "# append_jsonl(source_file_path, destination_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "<>:1: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "/tmp/ipykernel_2780267/3065421971.py:1: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  (\"\")(\"\")\n",
      "/tmp/ipykernel_2780267/3065421971.py:1: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  (\"\")(\"\")\n",
      "/tmp/ipykernel_2780267/3065421971.py:1: SyntaxWarning: 'str' object is not callable; perhaps you missed a comma?\n",
      "  (\"\")(\"\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    ";;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# Path to your JSONL file\n",
    "jsonl_file_path = \"data/output/sapbert_embedding_docs.jsonl\"\n",
    "# Path where the Pickle file will be saved\n",
    "pkl_file_path = \"data/output/sapbert_embedding_docs.pkl\"\n",
    "\n",
    "# List to hold the data from the JSONL file\n",
    "data = []\n",
    "\n",
    "# Read the JSONL file\n",
    "with open(jsonl_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Assuming each line is a separate JSON object\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Serialize the data to a Pickle file\n",
    "with open(pkl_file_path, \"wb\") as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def create_document(data):\n",
    "    try:\n",
    "        # print(data.keys())\n",
    "        # Check if 'page_content' exists in data, use an empty string as default if not\n",
    "        page_content = data.get(\"kwargs\", {}).get(\"page_content\", {})\n",
    "        # print(f\"page_content={page_content}\")\n",
    "        # Access 'metadata' safely\n",
    "        metadata = data.get(\"kwargs\", {}).get(\"metadata\", {})\n",
    "\n",
    "        # Create the Document object\n",
    "        document = Document(page_content=page_content, metadata=metadata)\n",
    "        return document\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document: {e}\")\n",
    "        # Return None or handle the error appropriately (perhaps re-raise the exception or log it)\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_docs_from_jsonl(file_path) -> list:\n",
    "    docs_dict = {}\n",
    "    with open(file_path, \"r\") as jsonl_file:\n",
    "        print(\"Opening file...\")\n",
    "        for line in tqdm(jsonl_file, desc=\"Loading Documents\"):\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                obj = Document(**data)\n",
    "            except Exception as e:\n",
    "                # print(f\"document object translated into Dictionary format\")\n",
    "                obj = create_document(data)\n",
    "            # print(f\"data={obj}\")\n",
    "            vocab = obj.metadata[\"vocab\"].lower()\n",
    "            if vocab in [\n",
    "                \"snomed\",\n",
    "                \"loinc\",\n",
    "                \"atc\",\n",
    "                \"ucum\",\n",
    "                \"rxnorm\",\n",
    "                \"omop extension\",\n",
    "                \"mesh\",\n",
    "                \"meddra\",\n",
    "            ]:\n",
    "                # Define a unique key based on page content and critical metadata\n",
    "                # This might include other metadata fields you consider critical for uniqueness\n",
    "                # Here we use a combination of page_content and a sorted JSON dump of metadata to ensure the key is unique and consistently formatted\n",
    "                key = (obj.page_content, json.dumps(obj.metadata, sort_keys=True))\n",
    "\n",
    "                # Only add to dictionary if it is truly unique\n",
    "                if key not in docs_dict:\n",
    "                    docs_dict[key] = obj\n",
    "\n",
    "    # Convert dictionary values to a sorted list to process documents in a specific order\n",
    "    sorted_docs = sorted(\n",
    "        docs_dict.values(), key=lambda doc: doc.metadata[\"vocab\"].lower()\n",
    "    )\n",
    "    print(f\"Total Unique Documents: {len(sorted_docs)}\")\n",
    "    return sorted_docs\n",
    "\n",
    "\n",
    "def save_docs_to_jsonl(array: Iterable[Document], file_path: str) -> None:\n",
    "    with open(file_path, \"w\") as jsonl_file:\n",
    "        for doc in array:\n",
    "            # print(doc.json())\n",
    "            jsonl_file.write(doc.json() + \"\\n\")\n",
    "\n",
    "\n",
    "save_docs_to_jsonl(\n",
    "    load_docs_from_jsonl(\n",
    "        \"data/output/sapbert_embedding_docs.jsonl\"\n",
    "    ),\n",
    "    \"data/output/sapbert_emb_docs_json.jsonl\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"./ncbi-disease/test_dictionary.txt\"\n",
    "with open(dict_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "cuis = []\n",
    "for l in lines:\n",
    "    cui = l.split(\"||\")[0]\n",
    "    cuis.append(cui)\n",
    "\n",
    "print(len(cuis))\n",
    "print(len(set(cuis)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"./ncbi-disease/train_dictionary.txt\"\n",
    "with open(dict_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "cuis_test = []\n",
    "for l in lines:\n",
    "    cui = l.split(\"||\")[0]\n",
    "    cuis_test.append(cui)\n",
    "print(len(cuis))\n",
    "print(len(set(cuis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"./bc5cdr-disease/test_dictionary.txt\"\n",
    "with open(dict_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "cuis = []\n",
    "for l in lines:\n",
    "    cui = l.split(\"||\")[0]\n",
    "    cuis.append(cui)\n",
    "print(len(cuis))\n",
    "print(len(set(cuis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"./medmentions/umls2017aa_reference_ont_lower.txt\"\n",
    "with open(dict_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "cuis = []\n",
    "for l in lines:\n",
    "    cui = l.split(\"||\")[0]\n",
    "    cuis.append(cui)\n",
    "print(len(cuis))\n",
    "print(len(set(cuis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"./askapatient/AskAPatient.dict.txt\"\n",
    "with open(dict_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "cuis = []\n",
    "for l in lines:\n",
    "    cui = l.split(\"||\")[0]\n",
    "    cuis.append(cui)\n",
    "print(len(cuis))\n",
    "print(len(set(cuis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"./askapatient/AskAPatient.dict.txt\"\n",
    "with open(dict_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "cuis = []\n",
    "for l in lines:\n",
    "    cui = l.split(\"||\")[0]\n",
    "    cuis.append(cui)\n",
    "print(len(cuis))\n",
    "print(len(set(cuis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sum = 0\n",
    "for i in range(10):\n",
    "    dict_path = \"./askapatient/AskAPatient.fold-\" + str(i) + \".validation.txt\"\n",
    "    with open(dict_path, \"rb\") as f:\n",
    "        lines = f.readlines()\n",
    "    cuis = []\n",
    "    for l in lines:\n",
    "        l = str(l)\n",
    "        cui = l.split(\"\\t\")[0]\n",
    "        cuis.append(cui)\n",
    "    len_sum += len(cuis)\n",
    "    # print (len(set(cuis)))\n",
    "print(len_sum / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = \"./cometa/COMETA_id_sf_dictionary.txt\"\n",
    "with open(dict_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "cuis = []\n",
    "for l in lines:\n",
    "    cui = l.split(\"||\")[0]\n",
    "    cuis.append(cui)\n",
    "print(len(cuis))\n",
    "print(len(set(cuis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npz data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(\n",
    "    \"data/eval_datasets/miid/miid-data.npz\", allow_pickle=True\n",
    ")\n",
    "\n",
    "# List array names and their shapes\n",
    "for array_name in data.files:\n",
    "    array = data[array_name]\n",
    "    print(f\"Array Name: {array_name}, Shape: {array.shape}, Dtype: {array.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the arrays\n",
    "mention_indices = data[\"mention_indices\"]\n",
    "mention_labels = data[\"mention_labels\"]\n",
    "mention_names = data[\"mention_names\"]\n",
    "concept_indices = data[\"concept_indices\"]\n",
    "concept_names = data[\"concept_names\"]\n",
    "\n",
    "# Convert to a DataFrame\n",
    "mentions_df = pd.DataFrame(\n",
    "    {\n",
    "        \"mention_index\": mention_indices,\n",
    "        \"mention_label\": mention_labels,\n",
    "        \"mention_name\": mention_names,\n",
    "    }\n",
    ")\n",
    "concept_df = pd.DataFrame(\n",
    "    {\"concept_index\": concept_indices, \"concept_name\": concept_names}\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Mentions DataFrame:\")\n",
    "print(mentions_df.head())\n",
    "mentions_df.to_csv(\n",
    "    \"data/eval_datasets/miid/miid-mentions.csv\", index=False\n",
    ")\n",
    "concept_df.to_csv(\n",
    "    \"data/eval_datasets/miid/miid-concepts.csv\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/eval_datasets/miid/miid-mentions.csv\")\n",
    "\n",
    "# Create 'bike_disease_queries.txt' with format 'mention_index||mention_label'\n",
    "bike_disease_queries = df[[\"mention_index\", \"mention_label\"]].astype(str)\n",
    "bike_disease_queries[\"combined\"] = (\n",
    "    bike_disease_queries[\"mention_index\"] + \"||\" + bike_disease_queries[\"mention_label\"]\n",
    ")\n",
    "bike_disease_queries[\"combined\"].to_csv(\n",
    "    \"BIKH_disease_queries.txt\", index=False, header=False\n",
    ")\n",
    "\n",
    "print(\"bike_disease_queries.txt created successfully.\")\n",
    "\n",
    "# Create 'dictionary.txt' with format 'mention_index||mention_name'\n",
    "dictionary = df[[\"mention_index\", \"mention_name\"]].astype(str)\n",
    "dictionary[\"combined\"] = dictionary[\"mention_index\"] + \"||\" + dictionary[\"mention_name\"]\n",
    "dictionary[\"combined\"].to_csv(\n",
    "    \"data/eval_datasets/miid/mimic_dictionary.txt\",\n",
    "    index=False,\n",
    "    header=False,\n",
    ")\n",
    "\n",
    "print(\"dictionary.txt created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def generate_files(\n",
    "    mention_csv_path, concept_csv_path, output_path_mention_queries, output_path_concept\n",
    "):\n",
    "    # Load concept data and create a lookup from label to id\n",
    "    concept_id_by_label = {}\n",
    "    with open(concept_csv_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Strip any whitespace and handle IDs consistently\n",
    "            concept_id, label = row[0].strip(), row[1].strip()\n",
    "            concept_id_by_label[label] = concept_id\n",
    "\n",
    "    # Process mention data and generate output files\n",
    "    mention_queries = []\n",
    "    concept_ids_used = set()\n",
    "\n",
    "    with open(mention_csv_path, mode=\"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Ensure data is stripped of any excess whitespace\n",
    "            mention_index, mention_label, mention_name = (\n",
    "                row[0].strip(),\n",
    "                row[1].strip(),\n",
    "                row[2].strip(),\n",
    "            )\n",
    "            if mention_label in concept_id_by_label:\n",
    "                concept_id = concept_id_by_label[mention_label]\n",
    "                mention_queries.append(f\"{concept_id}||{mention_name}\")\n",
    "                concept_ids_used.add(concept_id)\n",
    "\n",
    "    # Write mention queries to output file\n",
    "    with open(output_path_mention_queries, \"w\", encoding=\"utf-8\") as file:\n",
    "        for query in mention_queries:\n",
    "            file.write(query + \"\\n\")\n",
    "\n",
    "    # Write concept IDs and labels used to output file\n",
    "    with open(output_path_concept, \"w\", encoding=\"utf-8\") as file:\n",
    "        for label, concept_id in concept_id_by_label.items():\n",
    "            file.write(f\"{concept_id}||{label}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Example usage\n",
    "generate_files(\n",
    "    \"data/eval_datasets/miid/miid-mentions.csv\",\n",
    "    \"data/eval_datasets/miid/miid-concepts.csv\",\n",
    "    \"data/eval_datasets/miid/miid_queries.txt\",\n",
    "    \"data/eval_datasets/miid/miid_concepts.txt\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-0.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-1.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-2.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-3.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-4.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-5.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-6.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-7.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-8.test.txt\n",
      "Processing file: data/eval_datasets/askapatient/train_test/AskAPatient.fold-9.test.txt\n",
      "Combined queries written to: data/eval_datasets/askapatient/train_test/combined_queries.txt\n",
      "CUI to Label dictionary written to: data/eval_datasets/askapatient/train_test/test_dictionary.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "def process_directory(directory):\n",
    "    # Define the file pattern\n",
    "    pattern = os.path.join(directory, \"AskAPatient.fold-[0-9].test.txt\")\n",
    "\n",
    "    # Find all files matching the pattern\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No files found matching the pattern in directory: {directory}\")\n",
    "        return\n",
    "\n",
    "    cui_label_dict = {}\n",
    "    combined_queries = set()\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                for line_number, line in enumerate(file, 1):\n",
    "                    # Strip newline characters and split by tab\n",
    "                    parts = line.strip().split(\"\\t\")\n",
    "\n",
    "                    if len(parts) < 3:\n",
    "                        print(\n",
    "                            f\"Warning: Line {line_number} in {file_path} does not have at least 3 columns. Skipping.\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    cui, label, mention_text = parts[0], parts[1], parts[2]\n",
    "\n",
    "                    # Update the dictionary (assuming latest label overwrites previous ones)\n",
    "                    if cui != \"cui-less\":\n",
    "                        cui_label_dict[cui] = label\n",
    "                    if mention_text not in combined_queries:\n",
    "                        combined_queries.add(f\"{cui}||{mention_text}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Write combined_queries.txt\n",
    "    combined_queries_path = os.path.join(directory, \"combined_queries.txt\")\n",
    "    try:\n",
    "        with open(combined_queries_path, \"w\", encoding=\"utf-8\") as cq_file:\n",
    "            for query in combined_queries:\n",
    "                cq_file.write(f\"{query}\\n\")\n",
    "        print(f\"Combined queries written to: {combined_queries_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing combined_queries.txt: {e}\")\n",
    "\n",
    "    # Optionally, write the dictionary to a JSON file\n",
    "    # write the dictionary to a .txt file || seperated\n",
    "    try:\n",
    "        dict_path = os.path.join(directory, \"test_dictionary.txt\")\n",
    "        with open(dict_path, \"w\", encoding=\"utf-8\") as dict_file:\n",
    "            for cui, label in cui_label_dict.items():\n",
    "                dict_file.write(f\"{cui}||{label}\\n\")\n",
    "        print(f\"CUI to Label dictionary written to: {dict_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing cui_label_dict.json: {e}\")\n",
    "\n",
    "\n",
    "process_directory(\"data/eval_datasets/askapatient/train_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform mapped dictionary to reference set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"data/eval_datasets/Icare_data/CHECK-CHF_DATA_DICTIONARY_update_mapped.csv\",\n",
    "    dtype=str,\n",
    ")\n",
    "\n",
    "data = []\n",
    "for row in df.itertuples():\n",
    "    mention = \"\"\n",
    "    codes = \"\"\n",
    "    domain = \"\"\n",
    "    if pd.notna(row[10]):\n",
    "        domain = row[10].strip()\n",
    "    if pd.notna(row[2]) and pd.notna(row[9]):\n",
    "        mention = row[2].strip()\n",
    "\n",
    "        label_codes = row[9]\n",
    "\n",
    "        if pd.notna(row[13]):\n",
    "            label_codes = f\"{label_codes}|{row[13]}\"\n",
    "        item = f\"{label_codes}||{mention}||{domain}\"\n",
    "        data.append(item)\n",
    "\n",
    "    if pd.notna(row[5]):\n",
    "        categorical_values = row[5].split(\"|\")\n",
    "        if pd.notna(row[17]):\n",
    "            categorical_values_codes = row[17].split(\"|\")\n",
    "            min_ = min(len(categorical_values), len(categorical_values_codes))\n",
    "            for i in range(min_):\n",
    "                cat_mention = categorical_values[i].strip()\n",
    "                cat_code = categorical_values_codes[i].strip()\n",
    "                item = f\"{cat_code}||{cat_mention}||observation\"\n",
    "                data.append(item)\n",
    "\n",
    "    if pd.notna(row[4]):\n",
    "        unit = row[4]\n",
    "        if pd.notna(row[20]):\n",
    "            ucode = row[20].strip()\n",
    "            item = f\"{ucode}||{unit}||unit\"\n",
    "            data.append(item)\n",
    "    if pd.notna(row[6]):\n",
    "        visit = row[6].strip()\n",
    "        if pd.notna(row[13]):\n",
    "            visit_code = row[13].strip().split(\"|\")[-1]\n",
    "            item = f\"{visit_code}||{visit}||visit\"\n",
    "            data.append(item)\n",
    "print(len(data))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['46234437|45885250|45883167||ivabradina: dosage||drug',\n",
       " '8576||mg (miligram)| o.i.d| b.i.d||unit',\n",
       " '46234437|4162374|4145077||ivabradina: daily dose||drug',\n",
       " '46234437|4145077||Ivabradina: dosing frequency||drug',\n",
       " '46234437|4162374|4145077||ivabradina total daily dose||drug']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK-CHF Dictionary\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\n",
    "#     \"data/eval_datasets/Icare_data/updated_giss3+TIME_CHF+CHECK_CHF_mix_mapped.csv\",\n",
    "#     dtype=str,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dictionary\n",
    "# ground_truth = []\n",
    "# for row in df.itertuples():\n",
    "#     codes = \"\"\n",
    "#     labels = \"\"\n",
    "#     if pd.notna(row[7]):\n",
    "#         labels = row[7]\n",
    "#     if pd.notna(row[9]):\n",
    "#         codes = row[9]\n",
    "#     if pd.notna(row[11]):\n",
    "#         value = row[11]\n",
    "#         if \"|\" in row[11]:\n",
    "#             value = row[11].replace(\"|\", \"||\")\n",
    "#         labels = f\"{labels}||{value}\"\n",
    "#     if pd.notna(row[13]):\n",
    "#         codes = f\"{codes}|{row[13]}\"\n",
    "#     if pd.notna(row[15]):\n",
    "#         value = row[15]\n",
    "#         if \"|\" in row[15]:\n",
    "#             value = row[15].replace(\"|\", \"||\")\n",
    "#         labels = f\"{labels}||{value}\"\n",
    "#     if pd.notna(row[17]):\n",
    "#         codes = f\"{codes}|{row[17]}\"\n",
    "#     if pd.notna(row[18]):\n",
    "#         value = row[18]\n",
    "#         if \"|\" in row[18]:\n",
    "#             value = row[18].replace(\"|\", \"||\")\n",
    "#         labels = f\"{labels}||{value}\"\n",
    "#     if pd.notna(row[20]):\n",
    "#         codes = f\"{codes}|{row[20]}\"\n",
    "#     codes = codes.split(\"|\")\n",
    "#     labels = labels.split(\"||\")\n",
    "#     print(codes, labels)\n",
    "#     max_num = min(len(codes), len(labels))\n",
    "#     for i in range(max_num):\n",
    "#         ground_truth.append(f\"{codes[i]}||{labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data[:5]\n",
    "# ground_truth[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store in .txt file\n",
    "with open(\n",
    "    \"data/eval_datasets/Icare_data/reference_set_v2.txt\", \"a\"\n",
    ") as f:\n",
    "    for item in data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\n",
    "#     \"/workspace/other_tasks/syn_map/eval_dataset/custom_data/custom_dict.txt\", \"a\"\n",
    "# ) as f:\n",
    "#     for item in ground_truth:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_docs_from_jsonl\n\u001b[1;32m      3\u001b[0m docs \u001b[38;5;241m=\u001b[39m load_docs_from_jsonl(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/eval_datasets/original_bc5cdr-disease/test_dictionary_docs.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# I'll calculate token overlaps based on the provided code.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rag'"
     ]
    }
   ],
   "source": [
    "from rag.utils import load_docs_from_jsonl\n",
    "\n",
    "docs = load_docs_from_jsonl(\n",
    "    \"data/eval_datasets/original_bc5cdr-disease/test_dictionary_docs.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "# I'll calculate token overlaps based on the provided code.\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    # Tokenize the strings into sets of words\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "\n",
    "    # Calculate the intersection and union of the sets\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "\n",
    "    # Return the Jaccard similarity score\n",
    "    return intersection / union\n",
    "\n",
    "# Test input data (emulating 'docs' structure)\n",
    "docs = [\n",
    "    {\"metadata\": {\"label\": \"Hypertension\"}},\n",
    "    {\"metadata\": {\"label\": \"Pulmonary Hypertension\"}},\n",
    "    {\"metadata\": {\"label\": \"Systemic Hypertension\"}},\n",
    "    {\"metadata\": {\"label\": \"Hypertensive Disorder\"}},\n",
    "]\n",
    "\n",
    "# Calculating token overlap between labels\n",
    "similarities = []\n",
    "for doc in docs:\n",
    "    for doc_ in docs:\n",
    "        if doc == doc_:\n",
    "            continue\n",
    "        sim = jaccard_similarity(doc[\"metadata\"][\"label\"], doc_[\"metadata\"][\"label\"])\n",
    "        similarities.append((doc[\"metadata\"][\"label\"], doc_[\"metadata\"][\"label\"], sim))\n",
    "\n",
    "# Filtering those with similarity over 0.5\n",
    "high_similarity_pairs = [(label1, label2, sim) for label1, label2, sim in similarities if sim > 0.5]\n",
    "\n",
    "high_similarity_pairs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
