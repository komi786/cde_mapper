{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "STOP_WORDS = ['stop','start','combinations','combination','various combinations','various','left','right','blood','finding','finding status',\n",
    "              'status','extra','point in time','oral','product','oral product','several','types','several types','random','nominal',\n",
    "              'p time','quant','qual','quantitative','qualitative','ql','qn','quant','anti','antibodies','whole blood','serum','plasma','diseases',\n",
    "              'disorders','disorder','disease','lab test','measurements','lab tests','meas value','measurement','procedure','procedures',\n",
    "              'panel','ordinal','after','before','survey','level','levels','others','other','p dose','dose','dosage','frequency'\n",
    "]\n",
    "\n",
    "def add_relational_synonyms(concepts, relations):\n",
    "    relevant_relations = ['Tradename of', 'RxNorm - SNOMED eq', 'ATC - RxNorm', 'ATC - SNOMED eq', 'Has tradename']\n",
    "    \n",
    "    concept_synonyms = defaultdict(set)\n",
    "    \n",
    "    for _, row in tqdm(relations.iterrows(), total=relations.shape[0], desc='Processing relations'):\n",
    "        concept_id_1 = row['concept_id_1']\n",
    "        concept_id_2 = row['concept_id_2']\n",
    "        relationship_id = row['relationship_id']\n",
    "        \n",
    "        if relationship_id == 'Is a':\n",
    "            concept_name_2 = concepts.loc[concepts['concept_id'] == concept_id_2, 'concept_name'].values[0]\n",
    "            if concept_name_2:\n",
    "                concept_synonyms[concept_id_1].add(concept_name_2)\n",
    "                \n",
    "        elif relationship_id in relevant_relations:\n",
    "            concept_name_1 = concepts.loc[concepts['concept_id'] == concept_id_1, 'concept_name'].values[0]\n",
    "            concept_name_2 = concepts.loc[concepts['concept_id'] == concept_id_2, 'concept_name'].values[0]\n",
    "            \n",
    "            if concept_name_1 and concept_name_2:\n",
    "                concept_synonyms[concept_id_1].add(concept_name_2)\n",
    "                concept_synonyms[concept_id_2].add(concept_name_1)\n",
    "\n",
    "    # Update the concepts DataFrame\n",
    "    for concept_id, synonyms in concept_synonyms.items():\n",
    "        existing_synonyms = concepts.loc[concepts['concept_id'] == concept_id, 'concept_synonym_name'].values[0]\n",
    "        if pd.isna(existing_synonyms):\n",
    "            updated_synonyms = \";; \".join(synonyms)\n",
    "        else:\n",
    "            updated_synonyms = existing_synonyms + \";; \" + \";; \".join(synonyms)\n",
    "        concepts.loc[concepts['concept_id'] == concept_id, 'concept_synonym_name'] = updated_synonyms\n",
    "\n",
    "    concepts.to_csv(\"/workspace/rag_pipeline/data/input/omop_v5.4/hier_concepts.csv\", index=False)\n",
    "    \n",
    "    return concepts\n",
    "\n",
    "def load_data():\n",
    "    # Load the datasets\n",
    "    concepts = pd.read_csv('/workspace/rag_pipeline/data/gat_bert_data/omop_dir_1/concepts_chv.csv', dtype=str)\n",
    "    relations = pd.read_csv('/workspace/rag_pipeline/data/gat_bert_data/omop_dir_1/relationships.csv',  dtype=str)\n",
    "    concepts = add_relational_synonyms(concepts, relations)\n",
    "    return concepts\n",
    "\n",
    "def filter_synonyms(entity, entity_synonyms: set):\n",
    "    filtered_synonyms = {synonym for synonym in entity_synonyms if synonym not in STOP_WORDS}\n",
    "    return filtered_synonyms\n",
    "\n",
    "def build_ancestry(is_a_relations):\n",
    "    ancestry = {}\n",
    "    for _, row in is_a_relations.iterrows():\n",
    "        child, parent = row['concept_id_1'], row['concept_id_2']\n",
    "        ancestry.setdefault(child, []).append(parent)\n",
    "        if parent in ancestry:\n",
    "            ancestry[child].extend(ancestry[parent])\n",
    "    return ancestry\n",
    "\n",
    "def main():\n",
    "    concepts = load_data()\n",
    "    # Additional processing steps\n",
    "    # ancestry = build_ancestry(is_a_relations)\n",
    "    # create_title_text(concepts, synonyms, ancestry)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "STOP_WORDS = ['stop', 'start', 'combinations', 'combination', 'various combinations', 'various', 'left', 'right',\n",
    "              'blood', 'finding', 'finding status', 'status', 'extra', 'point in time', 'oral', 'product', 'oral product',\n",
    "              'several', 'types', 'several types', 'random', 'nominal', 'p time', 'quant', 'qual', 'quantitative',\n",
    "              'qualitative', 'ql', 'qn', 'quant', 'anti', 'antibodies', 'whole blood', 'serum', 'plasma', 'diseases',\n",
    "              'disorders', 'disorder', 'disease', 'lab test', 'measurements', 'lab tests', 'meas value', 'measurement',\n",
    "              'procedure', 'procedures', 'panel', 'ordinal', 'after', 'before', 'survey', 'level', 'levels', 'others',\n",
    "              'other', 'p dose', 'dose', 'dosage', 'frequency']\n",
    "\n",
    "RELEVANT_RELATIONS = ['RxNorm - SNOMED eq', 'ATC - RxNorm', 'ATC - SNOMED eq', 'Has tradename', 'Is a']\n",
    "\n",
    "def filter_relevant_relations(relations):\n",
    "    return relations[relations['relationship_id'].isin(RELEVANT_RELATIONS)]\n",
    "\n",
    "def add_relational_synonyms(concepts, relations, domain_id_priority='Drug'):\n",
    "    # Convert the DataFrame to dictionaries for faster lookups\n",
    "    concepts_dict = concepts.set_index('concept_id')['concept_name'].to_dict()\n",
    "    synonyms_dict = concepts.set_index('concept_id')['concept_synonym_name'].to_dict()\n",
    "    domain_dict = concepts.set_index('concept_id')['domain_id'].to_dict()\n",
    "\n",
    "    concept_synonyms = defaultdict(set)\n",
    "\n",
    "    for _, row in tqdm(relations.iterrows(), total=relations.shape[0], desc='Processing relations'):\n",
    "        concept_id_1 = row['concept_id_1']\n",
    "        concept_id_2 = row['concept_id_2']\n",
    "        relationship_id = row['relationship_id']\n",
    "\n",
    "        if relationship_id == 'Is a':\n",
    "            if concept_id_2 in concepts_dict:\n",
    "                concept_synonyms[concept_id_1].add(concepts_dict[concept_id_2])\n",
    "\n",
    "        elif (relationship_id.endswith('eq') or relationship_id == 'Has tradename') and domain_dict.get(concept_id_1) == domain_id_priority:\n",
    "            if concept_id_1 in concepts_dict and concept_id_2 in concepts_dict:\n",
    "                concept_synonyms[concept_id_1].add(concepts_dict[concept_id_2])\n",
    "                concept_synonyms[concept_id_2].add(concepts_dict[concept_id_1])\n",
    "\n",
    "        elif relationship_id in RELEVANT_RELATIONS:\n",
    "            if concept_id_1 in concepts_dict and concept_id_2 in concepts_dict:\n",
    "                concept_synonyms[concept_id_1].add(concepts_dict[concept_id_2])\n",
    "                concept_synonyms[concept_id_2].add(concepts_dict[concept_id_1])\n",
    "\n",
    "    for concept_id, synonyms in concept_synonyms.items():\n",
    "        existing_synonyms = synonyms_dict.get(concept_id, '')\n",
    "        existing_synonyms_set = set(existing_synonyms.split(';;')) if existing_synonyms else set()\n",
    "        updated_synonyms_set = existing_synonyms_set.union(synonyms)\n",
    "        updated_synonyms_set = {syn for syn in updated_synonyms_set if syn not in STOP_WORDS}\n",
    "        if len(updated_synonyms_set) > 10:\n",
    "            updated_synonyms_set = set(list(updated_synonyms_set)[:10])\n",
    "        updated_synonyms = ';; '.join(updated_synonyms_set)\n",
    "        concepts.at[concept_id, 'concept_synonym_name'] = updated_synonyms\n",
    "\n",
    "    return concepts\n",
    "\n",
    "def load_data_in_chunks(concepts_path, relations_path, chunk_size=5000):\n",
    "    concepts = pd.read_csv(concepts_path, dtype=str)\n",
    "    index = 0\n",
    "    for chunk in pd.read_csv(relations_path, dtype=str, chunksize=chunk_size, usecols=['concept_id_1', 'concept_id_2', 'relationship_id']):\n",
    "        filtered_chunk = filter_relevant_relations(chunk)\n",
    "        concepts = add_relational_synonyms(concepts, filtered_chunk)\n",
    "        concepts.to_csv(f\"/workspace/rag_pipeline/data/gat_bert_data/omop_dir_1/chunks/concept_chunk{index}.csv\", index=False)\n",
    "        index += 1\n",
    "    return concepts\n",
    "\n",
    "def filter_synonyms(entity, entity_synonyms: set):\n",
    "    return {synonym for synonym in entity_synonyms if synonym not in STOP_WORDS}\n",
    "\n",
    "def build_ancestry(is_a_relations):\n",
    "    ancestry = defaultdict(list)\n",
    "    for _, row in is_a_relations.iterrows():\n",
    "        child, parent = row['concept_id_1'], row['concept_id_2']\n",
    "        ancestry[child].append(parent)\n",
    "        if parent in ancestry:\n",
    "            ancestry[child].extend(ancestry[parent])\n",
    "    return ancestry\n",
    "\n",
    "def main():\n",
    "    concepts_path = '/workspace/rag_pipeline/data/gat_bert_data/omop_dir_1/concepts_chv.csv'\n",
    "    relations_path = '/workspace/rag_pipeline/data/gat_bert_data/omop_dir_1/filtered_relationships.csv'\n",
    "    concepts = load_data_in_chunks(concepts_path, relations_path)\n",
    "    concepts.to_csv(\"/workspace/rag_pipeline/data/gat_bert_data/omop_dir_1/updated_concepts.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_title_text(concepts, synonyms, ancestry):\n",
    "#     # Open a file to write the outputs\n",
    "#     # with open('/workspace/rag_pipeline/data/documents.txt', 'w') as file:\n",
    "#     outputs = []\n",
    "#     for _, concept in concepts.iterrows():\n",
    "#             concept_id = concept['concept_id']\n",
    "#             concept_name = concept['concept_name']\n",
    "#             concept_synonyms = set()\n",
    "#             if  not pd.isna(concept['concept_synonym_name']):\n",
    "#                 concept_synonyms = set(filter_synonyms(concept_name, set(concept['concept_synonym_name'].split(';;'))))\n",
    "#             # Get synonyms and related terms\n",
    "#             related_terms = synonyms[synonyms['concept_id'] == concept_id]\n",
    "#             if not related_terms.empty:\n",
    "#                 for col in ['uml_term', 'term', 'chv_term']:\n",
    "#                     if col in related_terms.columns:\n",
    "#                         terms = related_terms[col].dropna().str.split(';;').explode().dropna().unique()\n",
    "#                         concept_synonyms.update(terms)\n",
    "#             concept_synonyms.update([concept_name])  # Correctly update the set\n",
    "#             all_terms = ';;'.join(concept_synonyms)  # Join the updated set into a string\n",
    "#             meta_data = {\"concept_class\":concept['concept_class_id'], \"domain\":concept['domain_id'], \"code\":concept['concept_code'], \"cid\":concept['concept_id'],\n",
    "#                          \"vocabulary\":concept['vocabulary_id']}\n",
    "#             # Prepare the title with all ancestor entities\n",
    "#             ancestor_ids = ancestry.get(concept_id, [])\n",
    "#             ancestor_names = concepts[concepts['concept_id'].isin(ancestor_ids)]['concept_name'].tolist()\n",
    "#             title = ';;'.join(ancestor_names) if ancestor_names else concept_name\n",
    "\n",
    "#             # Prepare the text with concept details\n",
    "#             text = f\"{all_terms}\"\n",
    "\n",
    "#             # Write to file\n",
    "            \n",
    "#             text = f\"{title}\\t{all_terms}\\t{meta_data}\"\n",
    "#             outputs.append(text)\n",
    "#     with open('/workspace/rag_pipeline/data/output/documents.txt', 'w') as file:\n",
    "#         file.write(\"title\\ttext\\tmetadata\\n\")  # Writing the header\n",
    "#         file.write(\"\\n\".join(outputs))  # Writing all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Levenshtein import distance as edit_distance\n",
    "from graph_omop import Omop\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "DATA_DIR = '/workspace/rag_pipeline/data/input/omop_v5.4/extension_concepts'\n",
    "FIGSIZE = (12,6)\n",
    "TITLE_ARGS = {'fontsize': 20, 'y': 1.02}\n",
    "# sns.set(style=\"whitegrid\")\n",
    "omop_taxonomy = Omop(DATA_DIR, taxonomy=True)\n",
    "omop_taxonomy.load_concepts()\n",
    "#omop_taxonomy.build_domain_pair_for_training('/Users/komalgilani/Downloads/rag_pipeline_24May/data/input/unit1_concept_pairs_wth_semantic_type.csv',domain_id='unit', add_semantic_type=True)\n",
    "#omop_taxonomy.circulam_pair_for_training('/Users/komalgilani/Downloads/rag_pipeline_24May/data/input/train.txt', add_semantic_type=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Filter, PointStruct,PointIdsList\n",
    "import json\n",
    "\n",
    "def hash_content(content):\n",
    "    \"\"\"Generate a hash for the content to identify duplicates.\"\"\"\n",
    "    return hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "def fetch_all_points(client, collection_name, limit=500):\n",
    "    \"\"\"Fetch all points from the Qdrant collection.\"\"\"\n",
    "    all_points = []\n",
    "    offset = None  # Starting without an offset\n",
    "    while True:\n",
    "        scroll_result, offset = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=limit,\n",
    "            offset=offset,\n",
    "            with_payload=True,\n",
    "            with_vectors=True\n",
    "        )\n",
    "        if not scroll_result:\n",
    "            break\n",
    "        all_points.extend(scroll_result)\n",
    "        print(f\"len(all_points): {len(all_points)}\")\n",
    "        if offset is None:\n",
    "            break\n",
    "    return all_points\n",
    "\n",
    "def find_duplicates(points):\n",
    "    \"\"\"Find duplicates based on page_content and metadata.\"\"\"\n",
    "    seen = {}\n",
    "    duplicates = []\n",
    "    print(f\"Number of Points: {len(points)}\")\n",
    "    \n",
    "    for point in points:\n",
    "        page_content = point.payload.get('page_content')\n",
    "        if page_content and '<ENT>' in page_content:\n",
    "            page_content = page_content.split('<ENT>')[1].split('</ENT>')[0].split('||')[0]\n",
    "        metadata = point.payload.get('metadata')\n",
    "        unique_key = f\"{hash_content(page_content)}-{hash_content(json.dumps(metadata))}\"\n",
    "\n",
    "        if unique_key in seen:\n",
    "            duplicates.append(point)\n",
    "        else:\n",
    "            seen[unique_key] = point\n",
    "\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_points): 2000000\n",
      "len(all_points): 2008000\n",
      "len(all_points): 2016000\n",
      "len(all_points): 2024000\n",
      "len(all_points): 2032000\n",
      "len(all_points): 2040000\n",
      "len(all_points): 2048000\n",
      "len(all_points): 2056000\n",
      "len(all_points): 2064000\n",
      "len(all_points): 2072000\n",
      "len(all_points): 2080000\n",
      "len(all_points): 2088000\n",
      "len(all_points): 2096000\n",
      "len(all_points): 2104000\n",
      "len(all_points): 2112000\n",
      "len(all_points): 2120000\n",
      "len(all_points): 2128000\n",
      "len(all_points): 2136000\n",
      "len(all_points): 2144000\n",
      "len(all_points): 2152000\n",
      "len(all_points): 2160000\n",
      "len(all_points): 2168000\n",
      "len(all_points): 2176000\n",
      "len(all_points): 2184000\n",
      "len(all_points): 2192000\n",
      "len(all_points): 2200000\n",
      "len(all_points): 2208000\n",
      "len(all_points): 2216000\n",
      "len(all_points): 2224000\n",
      "len(all_points): 2232000\n",
      "len(all_points): 2240000\n",
      "len(all_points): 2248000\n",
      "len(all_points): 2256000\n",
      "len(all_points): 2264000\n",
      "len(all_points): 2272000\n",
      "len(all_points): 2280000\n",
      "len(all_points): 2288000\n",
      "len(all_points): 2296000\n",
      "len(all_points): 2304000\n",
      "len(all_points): 2312000\n",
      "len(all_points): 2320000\n",
      "len(all_points): 2328000\n",
      "len(all_points): 2336000\n",
      "len(all_points): 2344000\n",
      "len(all_points): 2352000\n",
      "len(all_points): 2360000\n",
      "len(all_points): 2368000\n",
      "len(all_points): 2376000\n",
      "len(all_points): 2384000\n",
      "len(all_points): 2392000\n",
      "len(all_points): 2400000\n",
      "len(all_points): 2408000\n",
      "len(all_points): 2416000\n",
      "len(all_points): 2424000\n",
      "len(all_points): 2432000\n",
      "len(all_points): 2440000\n",
      "len(all_points): 2448000\n",
      "len(all_points): 2456000\n",
      "len(all_points): 2464000\n",
      "len(all_points): 2472000\n",
      "len(all_points): 2480000\n",
      "len(all_points): 2488000\n",
      "len(all_points): 2496000\n",
      "len(all_points): 2504000\n",
      "len(all_points): 2512000\n",
      "len(all_points): 2520000\n",
      "len(all_points): 2528000\n",
      "len(all_points): 2536000\n",
      "len(all_points): 2544000\n",
      "len(all_points): 2552000\n",
      "len(all_points): 2560000\n",
      "len(all_points): 2568000\n",
      "len(all_points): 2576000\n",
      "len(all_points): 2584000\n",
      "len(all_points): 2592000\n",
      "len(all_points): 2600000\n",
      "len(all_points): 2608000\n",
      "len(all_points): 2616000\n",
      "len(all_points): 2624000\n",
      "len(all_points): 2632000\n",
      "len(all_points): 2640000\n",
      "len(all_points): 2648000\n",
      "len(all_points): 2656000\n",
      "len(all_points): 2664000\n",
      "len(all_points): 2672000\n",
      "len(all_points): 2680000\n",
      "len(all_points): 2688000\n",
      "len(all_points): 2696000\n",
      "len(all_points): 2704000\n",
      "len(all_points): 2712000\n",
      "len(all_points): 2720000\n",
      "len(all_points): 2728000\n",
      "len(all_points): 2736000\n",
      "len(all_points): 2744000\n",
      "len(all_points): 2752000\n",
      "len(all_points): 2760000\n",
      "len(all_points): 2768000\n",
      "len(all_points): 2776000\n",
      "len(all_points): 2784000\n",
      "len(all_points): 2792000\n",
      "len(all_points): 2800000\n",
      "len(all_points): 2808000\n",
      "len(all_points): 2816000\n",
      "len(all_points): 2824000\n",
      "len(all_points): 2832000\n",
      "len(all_points): 2840000\n",
      "len(all_points): 2848000\n",
      "len(all_points): 2856000\n",
      "len(all_points): 2864000\n",
      "len(all_points): 2872000\n",
      "len(all_points): 2880000\n",
      "len(all_points): 2888000\n",
      "len(all_points): 2896000\n",
      "len(all_points): 2901116\n",
      "Total points fetched: 2901116\n",
      "Number of Points: 2901116\n",
      "Total duplicates found: 0\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(url=\"http://qdrant:6333\")\n",
    "collection_name = \"SYNONYMS_MAPPING_SAP_ALL\"\n",
    "collection_name_1 = \"SYNONYMS_MAPPING_SAP_ALL_WITHOUT_DOMAIN\"\n",
    "print(client.get_collection(collection_name_1))\n",
    "\n",
    "limit = 8000  # Adjust based on your memory capacity\n",
    "total_deleted = 0\n",
    "\n",
    "points = fetch_all_points(client, collection_name, limit)\n",
    "print(f\"Total points fetched: {len(points)}\")\n",
    "duplicates = find_duplicates(points)\n",
    "print(f\"Total duplicates found: {len(duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates to delete.\n",
      "Total duplicates deleted: 0\n"
     ]
    }
   ],
   "source": [
    "def delete_duplicates(client, collection_name, duplicates):\n",
    "    \"\"\"Delete duplicate points from the collection.\"\"\"\n",
    "    ids_to_delete = [point.id for point in duplicates]\n",
    "    if ids_to_delete:\n",
    "        client.delete_vectors(collection_name, [\"dense_vector\"],ids_to_delete,wait=True)\n",
    "        client.delete(collection_name=collection_name, points_selector=ids_to_delete)\n",
    "        print(f\"Deleted {len(ids_to_delete)} duplicate points.\")\n",
    "    else:\n",
    "        print(\"No duplicates to delete.\")\n",
    "delete_duplicates(client, collection_name, duplicates)\n",
    "total_deleted += len(duplicates)\n",
    "\n",
    "print(f\"Total duplicates deleted: {total_deleted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Filter, PointStruct,PointIdsList\n",
    "client = QdrantClient(url=\"http://qdrant:6333\")\n",
    "collection_name_1 = \"SYNONYMS_MAPPING_SAP_ALL_WITHOUT_DOMAIN\"\n",
    "print(client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nDOMAIN: CONDITIONS\\n  \\n\\n \\n    MAX PAIRS = 10:\\n    Graph loaded from disk.\\n    Number of nodes in the graph: 2770116\\n    Total pairs processed: 520964\\n    Total domain ids processed: 96888\\n    Total synonyms processed : 520964\\n\\n\\nMAX PAIRS = 10:  \\n    Graph loaded from disk.\\n    Number of nodes in the graph: 2770116\\n    Total pairs processed: 595960\\n    Total domain ids processed: 134811\\n    Total synonyms processed : 595960\\n\\nDOMAIN: DRUG\\nMax pairs = 10\\n    Graph loaded from disk.\\n    Number of nodes in the graph: 2770116\\n    Total pairs processed: 3210755\\n    Total domain ids processed: 2062596\\n    Total synonyms processed : 3210755\\n    \\nDOMAIN: OBSERVATION\\nMAX PAIRS = 10\\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 642195\\nTotal domain ids processed: 155457\\nTotal synonyms processed : 642195\\n\\nDOMAIN: PROCEDURE\\nMax pairs = 10\\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 1006737\\nTotal domain ids processed: 276090\\nTotal synonyms processed : 1006737\\n\\nDOMAIN: DEVICE\\nMax pairs = 10\\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 46458\\nTotal domain ids processed: 14775\\nTotal synonyms processed : 46458\\n\\nDOMAIN: MEAS VALUE\\nMax pairs = 10\\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 33882\\nTotal domain ids processed: 23903\\nTotal synonyms processed : 33882\\nDOMAIN: Unit\\nMax pairs = 10\\n    \\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 187\\nTotal domain ids processed: 1031\\nTotal synonyms processed : 187\\nDOMAIN: MEASUREMENT\\n\\n\\nAdding synonym name as its own synonym. It means these concepts are synonyms of themselves because there synonym is not available\\nThe increase in the number of pairs is due to the addition of synonyms of the concepts.\\nDOMAIN: MEAS VALUE\\nMax pairs = 10\\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 35054\\nTotal domain ids processed: 23903\\nTotal synonyms processed : 35054\\nDOMAIN: DEVICE\\nMax pairs = 10\\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 46464\\nTotal domain ids processed: 14775\\nTotal synonyms processed : 46464\\n\\nDOMAIN: PROCEDURE\\nMax pairs = 10\\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 1006933\\nTotal domain ids processed: 276090\\nTotal synonyms processed : 1006933\\n\\nDOMAIN: OBSERVATION\\nMax pairs = 10\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 642951\\nTotal domain ids processed: 155457\\nTotal synonyms processed : 642951\\n\\nDOMAIN: DRUG\\nMax pairs = 10\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 3224390\\nTotal domain ids processed: 2062596\\nTotal synonyms processed : 3224390\\n\\nDOMAIN: MEASUREMENT\\nMax pairs = 10\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 596059\\nTotal domain ids processed: 134811\\nTotal synonyms processed : 596059\\n\\nDOMAIN: CONDITIONS\\nMax pairs = 10\\n\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 521098\\nTotal domain ids processed: 96888\\nTotal synonyms processed : 521098\\n\\nDOMAIN: Unit\\nMax pairs = 10\\nGraph loaded from disk.\\nNumber of nodes in the graph: 2770116\\nTotal pairs processed: 1031\\nTotal domain ids processed: 1031\\nTotal synonyms processed : 1031\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "DOMAIN: CONDITIONS\n",
    "  \n",
    "\n",
    " \n",
    "    MAX PAIRS = 10:\n",
    "    Graph loaded from disk.\n",
    "    Number of nodes in the graph: 2770116\n",
    "    Total pairs processed: 520964\n",
    "    Total domain ids processed: 96888\n",
    "    Total synonyms processed : 520964\n",
    "\n",
    "\n",
    "MAX PAIRS = 10:  \n",
    "    Graph loaded from disk.\n",
    "    Number of nodes in the graph: 2770116\n",
    "    Total pairs processed: 595960\n",
    "    Total domain ids processed: 134811\n",
    "    Total synonyms processed : 595960\n",
    "\n",
    "DOMAIN: DRUG\n",
    "Max pairs = 10\n",
    "    Graph loaded from disk.\n",
    "    Number of nodes in the graph: 2770116\n",
    "    Total pairs processed: 3210755\n",
    "    Total domain ids processed: 2062596\n",
    "    Total synonyms processed : 3210755\n",
    "    \n",
    "DOMAIN: OBSERVATION\n",
    "MAX PAIRS = 10\n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 642195\n",
    "Total domain ids processed: 155457\n",
    "Total synonyms processed : 642195\n",
    "\n",
    "DOMAIN: PROCEDURE\n",
    "Max pairs = 10\n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 1006737\n",
    "Total domain ids processed: 276090\n",
    "Total synonyms processed : 1006737\n",
    "\n",
    "DOMAIN: DEVICE\n",
    "Max pairs = 10\n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 46458\n",
    "Total domain ids processed: 14775\n",
    "Total synonyms processed : 46458\n",
    "\n",
    "DOMAIN: MEAS VALUE\n",
    "Max pairs = 10\n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 33882\n",
    "Total domain ids processed: 23903\n",
    "Total synonyms processed : 33882\n",
    "DOMAIN: Unit\n",
    "Max pairs = 10\n",
    "    \n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 187\n",
    "Total domain ids processed: 1031\n",
    "Total synonyms processed : 187\n",
    "DOMAIN: MEASUREMENT\n",
    "\n",
    "\n",
    "Adding synonym name as its own synonym. It means these concepts are synonyms of themselves because there synonym is not available\n",
    "The increase in the number of pairs is due to the addition of synonyms of the concepts.\n",
    "DOMAIN: MEAS VALUE\n",
    "Max pairs = 10\n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 35054\n",
    "Total domain ids processed: 23903\n",
    "Total synonyms processed : 35054\n",
    "DOMAIN: DEVICE\n",
    "Max pairs = 10\n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 46464\n",
    "Total domain ids processed: 14775\n",
    "Total synonyms processed : 46464\n",
    "\n",
    "DOMAIN: PROCEDURE\n",
    "Max pairs = 10\n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 1006933\n",
    "Total domain ids processed: 276090\n",
    "Total synonyms processed : 1006933\n",
    "\n",
    "DOMAIN: OBSERVATION\n",
    "Max pairs = 10\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 642951\n",
    "Total domain ids processed: 155457\n",
    "Total synonyms processed : 642951\n",
    "\n",
    "DOMAIN: DRUG\n",
    "Max pairs = 10\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 3224390\n",
    "Total domain ids processed: 2062596\n",
    "Total synonyms processed : 3224390\n",
    "\n",
    "DOMAIN: MEASUREMENT\n",
    "Max pairs = 10\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 596059\n",
    "Total domain ids processed: 134811\n",
    "Total synonyms processed : 596059\n",
    "\n",
    "DOMAIN: CONDITIONS\n",
    "Max pairs = 10\n",
    "\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 521098\n",
    "Total domain ids processed: 96888\n",
    "Total synonyms processed : 521098\n",
    "\n",
    "DOMAIN: Unit\n",
    "Max pairs = 10\n",
    "Graph loaded from disk.\n",
    "Number of nodes in the graph: 2770116\n",
    "Total pairs processed: 1031\n",
    "Total domain ids processed: 1031\n",
    "Total synonyms processed : 1031\n",
    "\"\"\"\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the data\n",
    "# domains = ['CONDITIONS', 'MEASUREMENT', 'DRUG', 'OBSERVATION', 'PROCEDURE', 'DEVICE', 'MEAS VALUE']\n",
    "# max_pairs = [10, 4, 5, 8, 10, 10, 10]\n",
    "# total_pairs = [520964, 360005, 306293, 519115, 3210755, 642195, 1006737]\n",
    "# total_domain_ids = [96888, 134811, 2062596, 155457, 276090, 14775, 23903]\n",
    "\n",
    "# # Create the bar chart\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.bar(domains, total_pairs)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.xlabel('Domain')\n",
    "# plt.ylabel('Number of Pairs')\n",
    "# plt.title('Number of Pairs for Each Domain')\n",
    "\n",
    "# # Display the chart\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged graph saved to /workspace/rag_pipeline/data/output/omop_bi_graph_all.pkl\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "# import os\n",
    "# import pickle\n",
    "# import networkx as nx\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def merge_graphs(graph_path1, graph_path2, output_path):\n",
    "#     \"\"\"\n",
    "#     Merge two graphs into a main graph.\n",
    "\n",
    "#     Args:\n",
    "#     graph_path1 (str): Path to the first graph (pickle file).\n",
    "#     graph_path2 (str): Path to the second graph (pickle file).\n",
    "#     output_path (str): Path to save the merged graph (pickle file).\n",
    "#     \"\"\"\n",
    "#     # Load the first graph\n",
    "#     with open(graph_path1, 'rb') as f:\n",
    "#         graph1 = pickle.load(f)\n",
    "    \n",
    "#     # Load the second graph\n",
    "#     with open(graph_path2, 'rb') as f:\n",
    "#         graph2 = pickle.load(f)\n",
    "    \n",
    "#     # Merge the two graphs\n",
    "#     merged_graph = nx.compose(graph1, graph2)\n",
    "    \n",
    "#     # Save the merged graph to disk\n",
    "#     with open(output_path, 'wb') as f:\n",
    "#         pickle.dump(merged_graph, f)\n",
    "    \n",
    "#     print(f\"Merged graph saved to {output_path}\")\n",
    "\n",
    "# merge_graphs('/workspace/rag_pipeline/data/input/omop_v5.4/extension_concepts/omop_bi_graph_extension.pkl',\n",
    "#                  '/workspace/rag_pipeline/data/output/omop_bi_graph_all.pkl', '/workspace/rag_pipeline/data/output/omop_bi_graph_all.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept dir = /workspace/rag_pipeline/data/input/omop_v5.4\n",
      "graph path = /workspace/rag_pipeline/data/input/omop_v5.4/omop_bi_graph_medra.pkl\n",
      "['MedDRA']\n",
      "Total entities=110802\n",
      "Total entities with synonyms =110802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110802it [00:10, 10076.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity not in graph = 0\n",
      "Entity not found in graph for hierarchy check = 1186303\n",
      "Entity not found in graph for Drugs check = 1120893\n",
      "Entity not found chv = 0\n",
      "Graph saved to disk.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
